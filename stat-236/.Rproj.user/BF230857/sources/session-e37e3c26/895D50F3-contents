### Relevant libraries
library(igraph)
library(parallel)

### Function to simulate from our hierarchical model:
# theta_i ~ i.i.d. Beta(a,b)
# Y_ij ~ indep Bern(theta_i * theta_j), i < j
sim <- function(a, b, n){
  theta <- rbeta(n, a, b)
  out <- matrix(1, n, n)
  for (i in 1:(n-1)) {
    for(j in (i+1):n){
      draw <- runif(1) < theta[i] * theta[j]
      out[i,j] <- draw
      out[j,i] <- draw
    }
  }
  return(out)
}

# Beta parameters
a <- 2
b <- 5

# Size of matrix
n <- 100

# Draw an example n x n matrix
m <- sim(a,b,n)

# Compute the mean of y_ij, for i < j
get_mu <- function(m){
  mean(m[upper.tri(m)])
}

# Compute the mean of y_ij * y_ik, samples that have one overlapping entry.
# Call this tau
get_tau <- function(m){
  n <- ncol(m)
  tau <- 0
  for (i in 1:(n-2)) {
    for (j in (i+1):(n-1)) {
      for(k in (j+1):n){
        tau <- tau + m[i,j]*m[i,k] + m[i,j]*m[j,k] + m[i,k]*m[j,k]
      }
    }
  }
  tau <- tau / (3 * choose(n,3))
  tau
}

# What are the theoretical values of these quantities?
true_mu <- (a/(a+b))^2
true_tau <- true_mu*a*(1+a)/((a+b)*(1+a+b))

# MoM estimate
mom <- function(mu, tau){
  a <- max(
    -(-tau* mu^2 +mu^4+sqrt(tau^2 * mu * (tau-mu^2)^2))/(tau-mu^2)^2,
    (-mu^4+mu^2 * tau+sqrt(mu *(mu^2-tau)^2 *tau^2))/(mu^2-tau)^2
  )
  
  b <- (a - a * sqrt(mu))/sqrt(mu)
  return(c(a,b))
}

# Monte Carlo
estim <- function(r, a, b, n){
  m <- sim(a,b,n)
  mu <- get_mu(m)
  tau <- get_tau(m)
  return(mom(mu,tau))
}

mc <- mclapply(1:1000, estim, a=2,b=5,n=100, mc.cores = 12)
as <- unlist(mc)[seq(1, 1999, 2)]
bs <- unlist(mc)[seq(2,2000,2)]
hist(as, main = "Histogram of alpha", xlab = "alpha")
mean(as)
hist(bs, main = "Histogram of beta", xlab = "beta")
mean(bs)

### Let's now do maximum likelihood inference using an approximate EM algorithm:
## Sufficient statistics are: s = sum_{i<j} Y_ij and t = sum_{i<j} (1-Y_ij)
sufficient <- function(m){
  c(
    sum(m[upper.tri(m)]),
    sum(1 - m[upper.tri(m)])
  )
}

# Expected log likelihood
ellik <- function(a, b, n, s, t){
  -n*lbeta(a,b) + 
    s*2*(digamma(a) - digamma(a+b)) -
    t*((a/(a+b))^2 + (a/(a+b))^4/2) +
    n*(a-1)*(digamma(a) - digamma(a+b)) +
    n*(b-1)*(digamma(b) - digamma(a+b))
}


ellik(200000,400000, n, s, t)

# Gradient of expectation of complete-data log-likelihood
gradient <- function(a, b, n, s, t){
  c(
    -2*t*a*b / (a+b)^3 + (2*s + n*(a-1))*trigamma(a) - (2*s + n*(a+b-2))*trigamma(a+b),
    2*t*a^2 / (a+b)^3 + n*(b - 1)*trigamma(b) - (2*s + n*(a+b-2))*trigamma(a+b)
  )
}

m <- sim(2,7, n)
suff <- sufficient(m)
s <- suff[1] 
t <- suff[2]

# E-M Algo
N_iter <- 10000000

params <- c(1,1)
eps <- 0.001

all_alphas <- 1
all_betas <- 1
for (i in 1:N_iter) {
  gr <- gradient(params[1], params[2], n, suff[1], suff[2])
  params <- params + eps*gr
  if(i%%1000 == 0){
    all_alphas <- c(all_alphas, params[1])
    all_betas <- c(all_betas, params[2])
  }
}

plot(all_alphas, main = "Estimate of alpha", xlab = "Iteration", ylab = "Value")
plot(all_betas, main = "Estimate of beta", xlab = "Iteration", ylab = "Value")

plot(all_alphas / (all_alphas + all_betas), main = "Estimate of alpha / (alpha + beta)", xlab = "Iteration", ylab = "Value")

### Fitting to empirical data
load("ke_data.RData")
data <- list(CoAuthor, Dolphin, Fan, Football, Karate, Polbooks, UKFaculty)
fits <- list()
for (i in 1:length(data)) {
  fits[[i]] <- mom(get_mu(data[[i]]), get_tau(data[[i]]))
}
fits

